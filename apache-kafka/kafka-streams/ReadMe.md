# Kafka Streams

카프카 내에서 데이터 처리(processing)과 transformation(변환)을 쉽게만들어주는것.
(내부 라이브러리)



# Concept
- 카프카 스트림은 변경 불가한 데이터의 연속이다. 정렬되어있고, 재생산가능하며 내결합성이있다.
- 카프카 프로세서는 입력 스트림(record by record)을 변환하고 그것으로 새로운 스트림을 만들어낸다. 시퀀스 자체는 변경할수 없기때문에 record는 스트림을 재생산하고 처리후에 record를 재조합한다.

1. Source processor : 카프카 토픽으로 부터 직접 데이터를 가져오는 특별한 프로세서이다. 부모 프로세서가 없으므로 데이터 변환이 필요하지않다.
2. Sink processor : 카프카 토픽으로 데이터를 직접 전송하는 프로세서이다. 자식 프로세서가 없음.

# Kstreams / KTable
1. Kstreams
```
전부 입력값이다.  (로그와 비슷)

실제로 존재 하지않으며 무제한으로 데이터 스트림

Topic에 데이터를 key, val값으로 추가시 최근 추가된 순서대로 데이터를 쌓아간다.


compact가 안된 토픽을 읽어 들인다.

새 데이터가 부분적인 정보나 트랜잭션인 경우
```

2. KTables
```
DB의 table과 유사(완전히 같은건 아님)

Upsert (업데이트 진행시 만족하는 튜플이 있으면 업데이트, 없다면 insert)

새로운 데이터의 val이 null로 입력될경우 이미 존재하는 튜플을 살제하게됨.

로그 compact가 된 토픽을 읽어 들인다.

데이터베이스 테이블같은 스스로 업데이트 된 구조가 필요할 경우
```
